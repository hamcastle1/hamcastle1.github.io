---
title: Today I Read Loftus (1993)
layout: post
---

Today I read a paper called "A picture is worth a thousand _p_ values: On the irrelevance of hypothesis testing in the microcomputer age" by Geoffrey Loftus. It was published in _Behavioral Research Methods_ in 1993. A link to the paper is available [here](http://drsmorey.org/bibtex/upload/Loftus:1993.pdf). 

<!--more-->

# Intro

My last post concerned a paper that focused on the problems involved in making decisions about the impact or "significance" of a behavioral research study based on _p_-values. Today I'm going to review a paper from a similar epoch that deals with the same problem, but goes into greater depth in discussing solutions/alternatives. 

# Summary

Loftus' primary concern is to encourage behavioral scientists to rely more completely on carefully constructed graphics than on hypothesis testing. He argues that one of the reasons this has not been done to date is because hypothesis testing, especially in the (unlikely, unreasonable) case of "normal-normal" testing for effects based on observed means, has simply been much easier to achieve than have adequate graphs. This has established a professional standard about the necessity of hypothesis tests in journal articles that has meant that even where such graphics have been produced, they have often been _removed_ and replaced with tables of results from statistical tests (apparently to save paper/ink?) The most important piece of information lost in this process, he suggests, is _power_, which is perhaps hard to understand when considered as a mathematical feature of data, but becomes much easier to grasp intuitively when visualized.

# Critique

An asshole could challenge the relevance of this paper to modern best practices in behavioral science research. Such a person would in the most technical sense be correct (the word "microcomputer" does appear in the title, after all), but would otherwise remain an asshole for failing to appreciate the spirit of this exercise. As I said in my last post, although it's true many in this field (even students) have learned the necessity of providing graphs that include both measures of central tendency _and_ spread in data, not _everyone_ does, and the demand from journals to always include results of statistical tests in potential papers has not in the intervening 20 years. Second, in an era of easy "big data", when even behavioral scientists are beginning to get in on the action, clear graphics to represent this information for increasingly complicated data sets are crucial -- and the skills needed to produce them are not widely taught or considered. It would be nice if more training programs included special courses or units for students in data visualization, but something tells me it would be unwise to hold my breath on this point...